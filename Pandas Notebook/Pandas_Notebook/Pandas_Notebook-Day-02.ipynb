{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Day 02\n",
    "# Instructor Turn - 01- Cleaning Data - üë©‚Äçüè´üßë‚Äçüè´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Name of the CSV file\n",
    "file = 'Resources/donors2008.csv'\n",
    "\n",
    "# The correct encoding must be used to read the CSV in pandas\n",
    "df = pd.read_csv(file, encoding=\"ISO-8859-1\") #UTF-8 universal use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>Employer</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Amount</th>\n",
       "      <th>FIELD8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron</td>\n",
       "      <td>Eugene</td>\n",
       "      <td>State Department</td>\n",
       "      <td>Dulles</td>\n",
       "      <td>VA</td>\n",
       "      <td>20189</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abadi</td>\n",
       "      <td>Barbara</td>\n",
       "      <td>Abadi &amp; Co.</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10021</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adamany</td>\n",
       "      <td>Anthony</td>\n",
       "      <td>Retired</td>\n",
       "      <td>Rockford</td>\n",
       "      <td>IL</td>\n",
       "      <td>61103</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams</td>\n",
       "      <td>Lorraine</td>\n",
       "      <td>Self</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10026</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adams</td>\n",
       "      <td>Marion</td>\n",
       "      <td>None</td>\n",
       "      <td>Exeter</td>\n",
       "      <td>NH</td>\n",
       "      <td>03833</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LastName FirstName          Employer      City State    Zip  Amount  FIELD8\n",
       "0    Aaron    Eugene  State Department    Dulles    VA  20189   500.0     NaN\n",
       "1    Abadi   Barbara       Abadi & Co.  New York    NY  10021   200.0     NaN\n",
       "2  Adamany   Anthony           Retired  Rockford    IL  61103   500.0     NaN\n",
       "3    Adams  Lorraine              Self  New York    NY  10026   200.0     NaN\n",
       "4    Adams    Marion              None    Exeter    NH  03833   100.0     NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview of the DataFrame\n",
    "# Note that FIELD8 is likely a meaningless column\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>Employer</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron</td>\n",
       "      <td>Eugene</td>\n",
       "      <td>State Department</td>\n",
       "      <td>Dulles</td>\n",
       "      <td>VA</td>\n",
       "      <td>20189</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abadi</td>\n",
       "      <td>Barbara</td>\n",
       "      <td>Abadi &amp; Co.</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10021</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adamany</td>\n",
       "      <td>Anthony</td>\n",
       "      <td>Retired</td>\n",
       "      <td>Rockford</td>\n",
       "      <td>IL</td>\n",
       "      <td>61103</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams</td>\n",
       "      <td>Lorraine</td>\n",
       "      <td>Self</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10026</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adams</td>\n",
       "      <td>Marion</td>\n",
       "      <td>None</td>\n",
       "      <td>Exeter</td>\n",
       "      <td>NH</td>\n",
       "      <td>03833</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LastName FirstName          Employer      City State    Zip  Amount\n",
       "0    Aaron    Eugene  State Department    Dulles    VA  20189   500.0\n",
       "1    Abadi   Barbara       Abadi & Co.  New York    NY  10021   200.0\n",
       "2  Adamany   Anthony           Retired  Rockford    IL  61103   500.0\n",
       "3    Adams  Lorraine              Self  New York    NY  10026   200.0\n",
       "4    Adams    Marion              None    Exeter    NH  03833   100.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete extraneous column\n",
    "del df['FIELD8']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LastName     1776\n",
       "FirstName    1776\n",
       "Employer     1743\n",
       "City         1776\n",
       "State        1776\n",
       "Zip          1776\n",
       "Amount       1776\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify incomplete rows\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with missing information\n",
    "df = df.dropna(how='any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LastName     1743\n",
       "FirstName    1743\n",
       "Employer     1743\n",
       "City         1743\n",
       "State        1743\n",
       "Zip          1743\n",
       "Amount       1743\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify dropped rows\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LastName      object\n",
       "FirstName     object\n",
       "Employer      object\n",
       "City          object\n",
       "State         object\n",
       "Zip           object\n",
       "Amount       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Amount column is the wrong data type. It should be numeric.\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pd.to_numeric() method to convert the datatype of the Amount column\n",
    "df['Amount'] = pd.to_numeric(df['Amount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the Amount column datatype has been made numeric\n",
    "df['Amount'].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None                              249\n",
       "Self                              241\n",
       "Retired                           126\n",
       "Self Employed                      39\n",
       "Self-Employed                      34\n",
       "                                 ... \n",
       "Park University                     1\n",
       "Cornerstones Of Care                1\n",
       "Walt Disney Company                 1\n",
       "Seastack Enterprises                1\n",
       "Midwest Acute Care Consultants      1\n",
       "Name: Employer, Length: 1011, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display an overview of the Employers column\n",
    "df['Employer'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Employer category. Replace 'Self Employed' and 'Self' with 'Self-Employed'\n",
    "df['Employer'] = df['Employer'].replace(\n",
    "    {'Self Employed': 'Self-Employed', 'Self': 'Self-Employed'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Self-Employed                       314\n",
       "None                                249\n",
       "Retired                             126\n",
       "Google                                6\n",
       "Not Employed                          4\n",
       "                                   ... \n",
       "Mutual Of Enumclaw Insurance Co.      1\n",
       "Tishman Realty                        1\n",
       "JDS Uniphase                          1\n",
       "Columbia College Chicago              1\n",
       "Midwest Acute Care Consultants        1\n",
       "Name: Employer, Length: 1009, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify clean-up.\n",
    "df['Employer'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Self-Employed                       314\n",
       "None                                249\n",
       "Retired                             126\n",
       "Unemployed                            8\n",
       "Google                                6\n",
       "                                   ... \n",
       "Mutual Of Enumclaw Insurance Co.      1\n",
       "Tishman Realty                        1\n",
       "JDS Uniphase                          1\n",
       "Columbia College Chicago              1\n",
       "Midwest Acute Care Consultants        1\n",
       "Name: Employer, Length: 1008, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Employer'] = df['Employer'].replace({'Not Employed': 'Unemployed'})\n",
    "df['Employer'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1743.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>640.124750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1242.343265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Amount\n",
       "count  1743.000000\n",
       "mean    640.124750\n",
       "std    1242.343265\n",
       "min       5.000000\n",
       "25%     200.000000\n",
       "50%     250.000000\n",
       "75%     500.000000\n",
       "max    5000.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a statistical overview\n",
    "# We can infer the maximum allowable individual contribution from 'max'\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Turn - 02 - Training Grounds - üë©‚Äçüéìüë®‚Äçüéì\n",
    "## Portland Crime\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Read in the csv using Pandas and print out the DataFrame that is returned.\n",
    "\n",
    "* Get a count of rows within the DataFrame in order to determine if there are any null values.\n",
    "\n",
    "* Drop the rows which contain null values.\n",
    "\n",
    "* Search through the \"Offense Type\" column and \"replace\" any similar values with one consistent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Crime Against</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Number of Records</th>\n",
       "      <th>Occur Date</th>\n",
       "      <th>Occur Month Year</th>\n",
       "      <th>Occur Time</th>\n",
       "      <th>Offense Category</th>\n",
       "      <th>Offense Count</th>\n",
       "      <th>Offense Type</th>\n",
       "      <th>Open Data Lat</th>\n",
       "      <th>Open Data Lon</th>\n",
       "      <th>Open Data X</th>\n",
       "      <th>Open Data Y</th>\n",
       "      <th>Report Date</th>\n",
       "      <th>Report Month Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>17-X4762181</td>\n",
       "      <td>Person</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/96</td>\n",
       "      <td>1/1/96</td>\n",
       "      <td>800</td>\n",
       "      <td>Sex Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>Rape</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/26/17</td>\n",
       "      <td>1/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>17-X4757824</td>\n",
       "      <td>Property</td>\n",
       "      <td>Centennial</td>\n",
       "      <td>1</td>\n",
       "      <td>1/20/00</td>\n",
       "      <td>1/1/00</td>\n",
       "      <td>1615</td>\n",
       "      <td>Fraud Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>Identity Theft</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/20/17</td>\n",
       "      <td>1/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200 BLOCK OF SE 78TH AVE</td>\n",
       "      <td>17-900367</td>\n",
       "      <td>Property</td>\n",
       "      <td>Montavilla</td>\n",
       "      <td>1</td>\n",
       "      <td>12/1/03</td>\n",
       "      <td>12/1/03</td>\n",
       "      <td>800</td>\n",
       "      <td>Fraud Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>False Pretenses/Swindle/Confidence Game</td>\n",
       "      <td>45.5207</td>\n",
       "      <td>-122.583</td>\n",
       "      <td>7668150.0</td>\n",
       "      <td>682825.0</td>\n",
       "      <td>1/9/17</td>\n",
       "      <td>1/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>17-X4748982</td>\n",
       "      <td>Property</td>\n",
       "      <td>Southwest Hills</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/10</td>\n",
       "      <td>1/1/10</td>\n",
       "      <td>0</td>\n",
       "      <td>Fraud Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>Identity Theft</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/5/17</td>\n",
       "      <td>1/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>17-X4748982</td>\n",
       "      <td>Property</td>\n",
       "      <td>Southwest Hills</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/10</td>\n",
       "      <td>1/1/10</td>\n",
       "      <td>0</td>\n",
       "      <td>Larceny Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>All Other Larceny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/5/17</td>\n",
       "      <td>1/1/17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Address  Case Number Crime Against     Neighborhood  \\\n",
       "0                       NaN  17-X4762181        Person              NaN   \n",
       "1                       NaN  17-X4757824      Property       Centennial   \n",
       "2  200 BLOCK OF SE 78TH AVE    17-900367      Property       Montavilla   \n",
       "3                       NaN  17-X4748982      Property  Southwest Hills   \n",
       "4                       NaN  17-X4748982      Property  Southwest Hills   \n",
       "\n",
       "   Number of Records Occur Date Occur Month Year  Occur Time  \\\n",
       "0                  1     1/1/96           1/1/96         800   \n",
       "1                  1    1/20/00           1/1/00        1615   \n",
       "2                  1    12/1/03          12/1/03         800   \n",
       "3                  1     1/1/10           1/1/10           0   \n",
       "4                  1     1/1/10           1/1/10           0   \n",
       "\n",
       "   Offense Category  Offense Count                             Offense Type  \\\n",
       "0      Sex Offenses              1                                     Rape   \n",
       "1    Fraud Offenses              1                           Identity Theft   \n",
       "2    Fraud Offenses              1  False Pretenses/Swindle/Confidence Game   \n",
       "3    Fraud Offenses              1                           Identity Theft   \n",
       "4  Larceny Offenses              1                        All Other Larceny   \n",
       "\n",
       "   Open Data Lat  Open Data Lon  Open Data X  Open Data Y Report Date  \\\n",
       "0            NaN            NaN          NaN          NaN     1/26/17   \n",
       "1            NaN            NaN          NaN          NaN     1/20/17   \n",
       "2        45.5207       -122.583    7668150.0     682825.0      1/9/17   \n",
       "3            NaN            NaN          NaN          NaN      1/5/17   \n",
       "4            NaN            NaN          NaN          NaN      1/5/17   \n",
       "\n",
       "  Report Month Year  \n",
       "0            1/1/17  \n",
       "1            1/1/17  \n",
       "2            1/1/17  \n",
       "3            1/1/17  \n",
       "4            1/1/17  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Reference the file where the CSV is located\n",
    "crime_csv_path = \"Resources/crime_incident_data2017.csv\"\n",
    "\n",
    "# Import the data into a Pandas DataFrame\n",
    "crime_df = pd.read_csv(crime_csv_path)\n",
    "crime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address              37365\n",
       "Case Number          41032\n",
       "Crime Against        41032\n",
       "Neighborhood         39712\n",
       "Number of Records    41032\n",
       "Occur Date           41032\n",
       "Occur Month Year     41032\n",
       "Occur Time           41032\n",
       "Offense Category     41032\n",
       "Offense Count        41032\n",
       "Offense Type         41032\n",
       "Open Data Lat        36712\n",
       "Open Data Lon        36712\n",
       "Open Data X          36712\n",
       "Open Data Y          36712\n",
       "Report Date          41032\n",
       "Report Month Year    41032\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for missing values\n",
    "crime_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null rows\n",
    "crime_df = crime_df.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address              36146\n",
       "Case Number          36146\n",
       "Crime Against        36146\n",
       "Neighborhood         36146\n",
       "Number of Records    36146\n",
       "Occur Date           36146\n",
       "Occur Month Year     36146\n",
       "Occur Time           36146\n",
       "Offense Category     36146\n",
       "Offense Count        36146\n",
       "Offense Type         36146\n",
       "Open Data Lat        36146\n",
       "Open Data Lon        36146\n",
       "Open Data X          36146\n",
       "Open Data Y          36146\n",
       "Report Date          36146\n",
       "Report Month Year    36146\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify counts\n",
    "crime_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Theft From Motor Vehicle                       6947\n",
       "Motor Vehicle Theft                            4689\n",
       "All Other Larceny                              4558\n",
       "Vandalism                                      3863\n",
       "Burglary                                       2824\n",
       "Shoplifting                                    2259\n",
       "Identity Theft                                 1794\n",
       "Simple Assault                                 1216\n",
       "Drug/Narcotic Violations                       1095\n",
       "Theft of Motor Vehicle Parts or Accessories    1073\n",
       "Intimidation                                    900\n",
       "Theft From Building                             895\n",
       "False Pretenses/Swindle/Confidence Game         870\n",
       "Aggravated Assault                              839\n",
       "Robbery                                         608\n",
       "Counterfeiting/Forgery                          448\n",
       "Weapons Law Violations                          266\n",
       "Credit Card/ATM Fraud                           226\n",
       "Arson                                           200\n",
       "Prostitution                                    145\n",
       "Pocket-Picking                                   94\n",
       "Purse-Snatching                                  89\n",
       "Embezzlement                                     73\n",
       "Stolen Property Offenses                         57\n",
       "Kidnapping/Abduction                             22\n",
       "Theft From Coin-Operated Machine or Device       20\n",
       "Hacking/Computer Invasion                        19\n",
       "Animal Cruelty                                   17\n",
       "Pornography/Obscene Material                     10\n",
       "Extortion/Blackmail                               8\n",
       "Assisting or Promoting Prostitution               7\n",
       "Drug Equipment Violations                         6\n",
       "Impersonation                                     4\n",
       "Wire Fraud                                        3\n",
       "Commercial Sex Acts                               1\n",
       "Welfare Fraud                                     1\n",
       "Name: Offense Type, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if there are any values with mispelled or similar values in \"Offense Type\"\n",
    "crime_df[\"Offense Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Crime Against</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Number of Records</th>\n",
       "      <th>Occur Date</th>\n",
       "      <th>Occur Month Year</th>\n",
       "      <th>Occur Time</th>\n",
       "      <th>Offense Category</th>\n",
       "      <th>Offense Count</th>\n",
       "      <th>Offense Type</th>\n",
       "      <th>Open Data Lat</th>\n",
       "      <th>Open Data Lon</th>\n",
       "      <th>Open Data X</th>\n",
       "      <th>Open Data Y</th>\n",
       "      <th>Report Date</th>\n",
       "      <th>Report Month Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200 BLOCK OF SE 78TH AVE</td>\n",
       "      <td>17-900367</td>\n",
       "      <td>Property</td>\n",
       "      <td>Montavilla</td>\n",
       "      <td>1</td>\n",
       "      <td>12/1/03</td>\n",
       "      <td>12/1/03</td>\n",
       "      <td>800</td>\n",
       "      <td>Fraud Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>False Pretenses/Swindle/Confidence Game</td>\n",
       "      <td>45.5207</td>\n",
       "      <td>-122.583</td>\n",
       "      <td>7668150.0</td>\n",
       "      <td>682825.0</td>\n",
       "      <td>1/9/17</td>\n",
       "      <td>1/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5400 BLOCK OF NE MALLORY AVE</td>\n",
       "      <td>17-900129</td>\n",
       "      <td>Property</td>\n",
       "      <td>King</td>\n",
       "      <td>1</td>\n",
       "      <td>11/28/10</td>\n",
       "      <td>11/1/10</td>\n",
       "      <td>1612</td>\n",
       "      <td>Fraud Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>Identity Theft</td>\n",
       "      <td>45.5625</td>\n",
       "      <td>-122.664</td>\n",
       "      <td>7647987.0</td>\n",
       "      <td>698581.0</td>\n",
       "      <td>1/3/17</td>\n",
       "      <td>1/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5000 BLOCK OF NE 19TH AVE</td>\n",
       "      <td>17-901079</td>\n",
       "      <td>Property</td>\n",
       "      <td>Vernon</td>\n",
       "      <td>1</td>\n",
       "      <td>11/8/13</td>\n",
       "      <td>11/1/13</td>\n",
       "      <td>1200</td>\n",
       "      <td>Fraud Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>False Pretenses/Swindle/Confidence Game</td>\n",
       "      <td>45.5594</td>\n",
       "      <td>-122.646</td>\n",
       "      <td>7652567.0</td>\n",
       "      <td>697337.0</td>\n",
       "      <td>1/26/17</td>\n",
       "      <td>1/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5000 BLOCK OF NE 19TH AVE</td>\n",
       "      <td>17-901079</td>\n",
       "      <td>Property</td>\n",
       "      <td>Vernon</td>\n",
       "      <td>1</td>\n",
       "      <td>11/8/13</td>\n",
       "      <td>11/1/13</td>\n",
       "      <td>1200</td>\n",
       "      <td>Fraud Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>Identity Theft</td>\n",
       "      <td>45.5594</td>\n",
       "      <td>-122.646</td>\n",
       "      <td>7652567.0</td>\n",
       "      <td>697337.0</td>\n",
       "      <td>1/26/17</td>\n",
       "      <td>1/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12000 BLOCK OF SE PINE ST</td>\n",
       "      <td>17-900253</td>\n",
       "      <td>Property</td>\n",
       "      <td>Hazelwood</td>\n",
       "      <td>1</td>\n",
       "      <td>1/6/14</td>\n",
       "      <td>1/1/14</td>\n",
       "      <td>805</td>\n",
       "      <td>Fraud Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>Credit Card/ATM Fraud</td>\n",
       "      <td>45.5204</td>\n",
       "      <td>-122.539</td>\n",
       "      <td>7679522.0</td>\n",
       "      <td>682404.0</td>\n",
       "      <td>1/6/17</td>\n",
       "      <td>1/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41026</th>\n",
       "      <td>10200 BLOCK OF NE SANDY BLVD</td>\n",
       "      <td>17-286419</td>\n",
       "      <td>Society</td>\n",
       "      <td>Parkrose</td>\n",
       "      <td>1</td>\n",
       "      <td>8/31/17</td>\n",
       "      <td>8/1/17</td>\n",
       "      <td>2005</td>\n",
       "      <td>Drug/Narcotic Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>Drug/Narcotic Violations</td>\n",
       "      <td>45.5591</td>\n",
       "      <td>-122.557</td>\n",
       "      <td>7675224.0</td>\n",
       "      <td>696649.0</td>\n",
       "      <td>8/31/17</td>\n",
       "      <td>8/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41027</th>\n",
       "      <td>8800 BLOCK OF NE SANDY BLVD</td>\n",
       "      <td>17-285386</td>\n",
       "      <td>Society</td>\n",
       "      <td>Sumner</td>\n",
       "      <td>1</td>\n",
       "      <td>8/31/17</td>\n",
       "      <td>8/1/17</td>\n",
       "      <td>535</td>\n",
       "      <td>Drug/Narcotic Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>Drug/Narcotic Violations</td>\n",
       "      <td>45.5554</td>\n",
       "      <td>-122.571</td>\n",
       "      <td>7671560.0</td>\n",
       "      <td>695399.0</td>\n",
       "      <td>8/31/17</td>\n",
       "      <td>8/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41028</th>\n",
       "      <td>9700 BLOCK OF SE STARK ST</td>\n",
       "      <td>17-286082</td>\n",
       "      <td>Society</td>\n",
       "      <td>Hazelwood</td>\n",
       "      <td>1</td>\n",
       "      <td>8/31/17</td>\n",
       "      <td>8/1/17</td>\n",
       "      <td>1619</td>\n",
       "      <td>Prostitution Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>Prostitution</td>\n",
       "      <td>45.5191</td>\n",
       "      <td>-122.563</td>\n",
       "      <td>7673292.0</td>\n",
       "      <td>682101.0</td>\n",
       "      <td>8/31/17</td>\n",
       "      <td>8/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41029</th>\n",
       "      <td>9700 BLOCK OF SE STARK ST</td>\n",
       "      <td>17-286413</td>\n",
       "      <td>Society</td>\n",
       "      <td>Hazelwood</td>\n",
       "      <td>1</td>\n",
       "      <td>8/31/17</td>\n",
       "      <td>8/1/17</td>\n",
       "      <td>1959</td>\n",
       "      <td>Prostitution Offenses</td>\n",
       "      <td>1</td>\n",
       "      <td>Prostitution</td>\n",
       "      <td>45.5191</td>\n",
       "      <td>-122.563</td>\n",
       "      <td>7673292.0</td>\n",
       "      <td>682101.0</td>\n",
       "      <td>8/31/17</td>\n",
       "      <td>8/1/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41031</th>\n",
       "      <td>8000 BLOCK OF SE POWELL BLVD</td>\n",
       "      <td>17-286659</td>\n",
       "      <td>Society</td>\n",
       "      <td>Foster-Powell</td>\n",
       "      <td>1</td>\n",
       "      <td>8/31/17</td>\n",
       "      <td>8/1/17</td>\n",
       "      <td>2354</td>\n",
       "      <td>Weapon Law Violations</td>\n",
       "      <td>1</td>\n",
       "      <td>Weapons Law Violations</td>\n",
       "      <td>45.4974</td>\n",
       "      <td>-122.580</td>\n",
       "      <td>7668725.0</td>\n",
       "      <td>674298.0</td>\n",
       "      <td>8/31/17</td>\n",
       "      <td>8/1/17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36146 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Address Case Number Crime Against   Neighborhood  \\\n",
       "2          200 BLOCK OF SE 78TH AVE   17-900367      Property     Montavilla   \n",
       "5      5400 BLOCK OF NE MALLORY AVE   17-900129      Property           King   \n",
       "6         5000 BLOCK OF NE 19TH AVE   17-901079      Property         Vernon   \n",
       "7         5000 BLOCK OF NE 19TH AVE   17-901079      Property         Vernon   \n",
       "8         12000 BLOCK OF SE PINE ST   17-900253      Property      Hazelwood   \n",
       "...                             ...         ...           ...            ...   \n",
       "41026  10200 BLOCK OF NE SANDY BLVD   17-286419       Society       Parkrose   \n",
       "41027   8800 BLOCK OF NE SANDY BLVD   17-285386       Society         Sumner   \n",
       "41028     9700 BLOCK OF SE STARK ST   17-286082       Society      Hazelwood   \n",
       "41029     9700 BLOCK OF SE STARK ST   17-286413       Society      Hazelwood   \n",
       "41031  8000 BLOCK OF SE POWELL BLVD   17-286659       Society  Foster-Powell   \n",
       "\n",
       "       Number of Records Occur Date Occur Month Year  Occur Time  \\\n",
       "2                      1    12/1/03          12/1/03         800   \n",
       "5                      1   11/28/10          11/1/10        1612   \n",
       "6                      1    11/8/13          11/1/13        1200   \n",
       "7                      1    11/8/13          11/1/13        1200   \n",
       "8                      1     1/6/14           1/1/14         805   \n",
       "...                  ...        ...              ...         ...   \n",
       "41026                  1    8/31/17           8/1/17        2005   \n",
       "41027                  1    8/31/17           8/1/17         535   \n",
       "41028                  1    8/31/17           8/1/17        1619   \n",
       "41029                  1    8/31/17           8/1/17        1959   \n",
       "41031                  1    8/31/17           8/1/17        2354   \n",
       "\n",
       "             Offense Category  Offense Count  \\\n",
       "2              Fraud Offenses              1   \n",
       "5              Fraud Offenses              1   \n",
       "6              Fraud Offenses              1   \n",
       "7              Fraud Offenses              1   \n",
       "8              Fraud Offenses              1   \n",
       "...                       ...            ...   \n",
       "41026  Drug/Narcotic Offenses              1   \n",
       "41027  Drug/Narcotic Offenses              1   \n",
       "41028   Prostitution Offenses              1   \n",
       "41029   Prostitution Offenses              1   \n",
       "41031   Weapon Law Violations              1   \n",
       "\n",
       "                                  Offense Type  Open Data Lat  Open Data Lon  \\\n",
       "2      False Pretenses/Swindle/Confidence Game        45.5207       -122.583   \n",
       "5                               Identity Theft        45.5625       -122.664   \n",
       "6      False Pretenses/Swindle/Confidence Game        45.5594       -122.646   \n",
       "7                               Identity Theft        45.5594       -122.646   \n",
       "8                        Credit Card/ATM Fraud        45.5204       -122.539   \n",
       "...                                        ...            ...            ...   \n",
       "41026                 Drug/Narcotic Violations        45.5591       -122.557   \n",
       "41027                 Drug/Narcotic Violations        45.5554       -122.571   \n",
       "41028                             Prostitution        45.5191       -122.563   \n",
       "41029                             Prostitution        45.5191       -122.563   \n",
       "41031                   Weapons Law Violations        45.4974       -122.580   \n",
       "\n",
       "       Open Data X  Open Data Y Report Date Report Month Year  \n",
       "2        7668150.0     682825.0      1/9/17            1/1/17  \n",
       "5        7647987.0     698581.0      1/3/17            1/1/17  \n",
       "6        7652567.0     697337.0     1/26/17            1/1/17  \n",
       "7        7652567.0     697337.0     1/26/17            1/1/17  \n",
       "8        7679522.0     682404.0      1/6/17            1/1/17  \n",
       "...            ...          ...         ...               ...  \n",
       "41026    7675224.0     696649.0     8/31/17            8/1/17  \n",
       "41027    7671560.0     695399.0     8/31/17            8/1/17  \n",
       "41028    7673292.0     682101.0     8/31/17            8/1/17  \n",
       "41029    7673292.0     682101.0     8/31/17            8/1/17  \n",
       "41031    7668725.0     674298.0     8/31/17            8/1/17  \n",
       "\n",
       "[36146 rows x 17 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining similar offenses together\n",
    "crime_df = crime_df.replace(\n",
    "    {\"Commercial Sex Acts\": \"Prostitution\", \"Assisting or Promoting Prostitution\": \"Prostitution\"})\n",
    "crime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Theft From Motor Vehicle                       6947\n",
       "Motor Vehicle Theft                            4689\n",
       "All Other Larceny                              4558\n",
       "Vandalism                                      3863\n",
       "Burglary                                       2824\n",
       "Shoplifting                                    2259\n",
       "Identity Theft                                 1794\n",
       "Simple Assault                                 1216\n",
       "Drug/Narcotic Violations                       1095\n",
       "Theft of Motor Vehicle Parts or Accessories    1073\n",
       "Intimidation                                    900\n",
       "Theft From Building                             895\n",
       "False Pretenses/Swindle/Confidence Game         870\n",
       "Aggravated Assault                              839\n",
       "Robbery                                         608\n",
       "Counterfeiting/Forgery                          448\n",
       "Weapons Law Violations                          266\n",
       "Credit Card/ATM Fraud                           226\n",
       "Arson                                           200\n",
       "Prostitution                                    153\n",
       "Pocket-Picking                                   94\n",
       "Purse-Snatching                                  89\n",
       "Embezzlement                                     73\n",
       "Stolen Property Offenses                         57\n",
       "Kidnapping/Abduction                             22\n",
       "Theft From Coin-Operated Machine or Device       20\n",
       "Hacking/Computer Invasion                        19\n",
       "Animal Cruelty                                   17\n",
       "Pornography/Obscene Material                     10\n",
       "Extortion/Blackmail                               8\n",
       "Drug Equipment Violations                         6\n",
       "Impersonation                                     4\n",
       "Wire Fraud                                        3\n",
       "Welfare Fraud                                     1\n",
       "Name: Offense Type, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if you comnbined similar offenses correctly in \"Offense Type\".\n",
    "crime_df[\"Offense Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Property    31622\n",
       "Person       2978\n",
       "Society      1546\n",
       "Name: Crime Against, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of crimes against property, society, and person.\n",
    "crime_df[\"Crime Against\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Activity 02 Solution ‚úÖ</strong></summary>\n",
    "    \n",
    "```python\n",
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Reference the file where the CSV is located\n",
    "crime_csv_path = \"Resources/crime_incident_data2017.csv\"\n",
    "\n",
    "# Import the data into a Pandas DataFrame\n",
    "crime_df = pd.read_csv(crime_csv_path)\n",
    "crime_df\n",
    "\n",
    "# look for missing values\n",
    "crime_df.count()\n",
    "\n",
    "# drop null rows\n",
    "no_null_crime_df = crime_df.dropna(how='any')\n",
    "\n",
    "# verify counts\n",
    "no_null_crime_df.count()\n",
    "\n",
    "# Check to see if there are any values with mispelled or similar values in \"Offense Type\"\n",
    "no_null_crime_df[\"Offense Type\"].value_counts()\n",
    "\n",
    "# Combining similar offenses together\n",
    "no_null_crime_df = no_null_crime_df.replace(\n",
    "    {\"Commercial Sex Acts\": \"Prostitution\", \"Assisting or Promoting Prostitution\": \"Prostitution\"})\n",
    "no_null_crime_df\n",
    "\n",
    "# Check to see if you comnbined similar offenses correctly in \"Offense Type\".\n",
    "no_null_crime_df[\"Offense Type\"].value_counts()\n",
    "\n",
    "# Get the number of crimes against property, society, and person.\n",
    "no_null_crime_df[\"Crime Against\"].value_counts()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn - 03 - Loc And Iloc - üë©‚Äçüè´üßë‚Äçüè´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>Phone Number</th>\n",
       "      <th>Time zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Peter</td>\n",
       "      <td>Richardson</td>\n",
       "      <td>7-(789)867-9023</td>\n",
       "      <td>Europe/Moscow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Janice</td>\n",
       "      <td>Berry</td>\n",
       "      <td>86-(614)973-1727</td>\n",
       "      <td>Asia/Harbin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Andrea</td>\n",
       "      <td>Hudson</td>\n",
       "      <td>86-(918)527-6371</td>\n",
       "      <td>Asia/Shanghai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>Mcdonald</td>\n",
       "      <td>420-(553)779-7783</td>\n",
       "      <td>Europe/Prague</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Kathy</td>\n",
       "      <td>Morales</td>\n",
       "      <td>351-(720)541-2124</td>\n",
       "      <td>Europe/Lisbon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id first_name   last_name       Phone Number      Time zone\n",
       "0   1      Peter  Richardson    7-(789)867-9023  Europe/Moscow\n",
       "1   2     Janice       Berry   86-(614)973-1727    Asia/Harbin\n",
       "2   3     Andrea      Hudson   86-(918)527-6371  Asia/Shanghai\n",
       "3   4     Arthur    Mcdonald  420-(553)779-7783  Europe/Prague\n",
       "4   5      Kathy     Morales  351-(720)541-2124  Europe/Lisbon"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = \"Resources/sampleData.csv\"\n",
    "\n",
    "original_df = pd.read_csv(file)\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>Phone Number</th>\n",
       "      <th>Time zone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Richardson</th>\n",
       "      <td>1</td>\n",
       "      <td>Peter</td>\n",
       "      <td>7-(789)867-9023</td>\n",
       "      <td>Europe/Moscow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Berry</th>\n",
       "      <td>2</td>\n",
       "      <td>Janice</td>\n",
       "      <td>86-(614)973-1727</td>\n",
       "      <td>Asia/Harbin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hudson</th>\n",
       "      <td>3</td>\n",
       "      <td>Andrea</td>\n",
       "      <td>86-(918)527-6371</td>\n",
       "      <td>Asia/Shanghai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mcdonald</th>\n",
       "      <td>4</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>420-(553)779-7783</td>\n",
       "      <td>Europe/Prague</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Morales</th>\n",
       "      <td>5</td>\n",
       "      <td>Kathy</td>\n",
       "      <td>351-(720)541-2124</td>\n",
       "      <td>Europe/Lisbon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id first_name       Phone Number      Time zone\n",
       "last_name                                                  \n",
       "Richardson   1      Peter    7-(789)867-9023  Europe/Moscow\n",
       "Berry        2     Janice   86-(614)973-1727    Asia/Harbin\n",
       "Hudson       3     Andrea   86-(918)527-6371  Asia/Shanghai\n",
       "Mcdonald     4     Arthur  420-(553)779-7783  Europe/Prague\n",
       "Morales      5      Kathy  351-(720)541-2124  Europe/Lisbon"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set new index to last_name\n",
    "df = original_df.set_index(\"last_name\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Loc: 86-(614)973-1727\n"
     ]
    }
   ],
   "source": [
    "# Grab the data contained within the \"Berry\" row and the \"Phone Number\" column\n",
    "berry_phone = df.loc[\"Berry\", \"Phone Number\"]\n",
    "print(\"Using Loc: \" + berry_phone)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Iloc: 86-(614)973-1727\n"
     ]
    }
   ],
   "source": [
    "also_berry_phone = df.iloc[1, 2]\n",
    "print(\"Using Iloc: \" + also_berry_phone)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the first five rows of data and the columns from \"id\" to \"Phone Number\"\n",
    "# The problem with using \"last_name\" as the index is that the values are not unique so duplicates are returned\n",
    "# If there are duplicates and loc[] is being used, Pandas will return an error\n",
    "richardson_to_morales = df.loc[[\"Richardson\", \"Berry\", \"Hudson\",\n",
    "                                \"Mcdonald\", \"Morales\"], [\"id\", \"first_name\", \"Phone Number\"]]\n",
    "richardson_to_morales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using iloc[] will not find duplicates since a numeric index is always unique\n",
    "also_richardson_to_morales = df.iloc[0:4, 0:3]\n",
    "also_richardson_to_morales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will select all rows for columns `first_name` and `Phone Number`\n",
    "df.loc[:, [\"first_name\", \"Phone Number\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following logic test/conditional statement returns a series of boolean values\n",
    "named_billy = df[\"first_name\"] == \"Billy\"\n",
    "named_billy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loc and Iloc also allow for conditional statments to filter rows of data\n",
    "# using Loc on the logic test above only returns rows where the result is True\n",
    "only_billys = df.loc[df[\"first_name\"] == \"Billy\", :]\n",
    "only_billys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions can be set to narrow down or widen the filter\n",
    "only_billy_and_peter = df.loc[(df[\"first_name\"] == \"Billy\") | (\n",
    "    df[\"first_name\"] == \"Peter\"), :]\n",
    "\n",
    "only_billy_and_peter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Turn - 04 - Good Movies - üë©‚Äçüéìüë®‚Äçüéì\n",
    "## Good Movies\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Use Pandas to load and display the CSV provided in `Resources`.\n",
    "\n",
    "* List all the columns in the data set.\n",
    "\n",
    "* We're only interested in IMDb data, so create a new table that takes the Film and all the columns relating to IMDB.\n",
    "\n",
    "* Filter out only the good movies‚Äîi.e., any film with an IMDb score greater than or equal to 7 and remove the norm ratings.\n",
    "\n",
    "* Find less popular movies that you may not have heard about - i.e., anything with under 20K votes\n",
    "\n",
    "Data Source:[Moving Rating Dataset](https://github.com/fivethirtyeight/data/blob/master/fandango/fandango_score_comparison.csv)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencie\n",
    "import pandas as pd\n",
    "\n",
    "# Load in file\n",
    "movie_file = \"Resources/movie_scores.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILM</th>\n",
       "      <th>RottenTomatoes</th>\n",
       "      <th>RottenTomatoes_User</th>\n",
       "      <th>Metacritic</th>\n",
       "      <th>Metacritic_User</th>\n",
       "      <th>IMDB</th>\n",
       "      <th>Fandango_Stars</th>\n",
       "      <th>Fandango_Ratingvalue</th>\n",
       "      <th>RT_norm</th>\n",
       "      <th>RT_user_norm</th>\n",
       "      <th>...</th>\n",
       "      <th>IMDB_norm</th>\n",
       "      <th>RT_norm_round</th>\n",
       "      <th>RT_user_norm_round</th>\n",
       "      <th>Metacritic_norm_round</th>\n",
       "      <th>Metacritic_user_norm_round</th>\n",
       "      <th>IMDB_norm_round</th>\n",
       "      <th>Metacritic_user_vote_count</th>\n",
       "      <th>IMDB_user_vote_count</th>\n",
       "      <th>Fandango_votes</th>\n",
       "      <th>Fandango_Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avengers: Age of Ultron (2015)</td>\n",
       "      <td>74</td>\n",
       "      <td>86</td>\n",
       "      <td>66</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1330</td>\n",
       "      <td>271107</td>\n",
       "      <td>14846</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cinderella (2015)</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>67</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.55</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>249</td>\n",
       "      <td>65709</td>\n",
       "      <td>12640</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ant-Man (2015)</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>64</td>\n",
       "      <td>8.1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>3.90</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>627</td>\n",
       "      <td>103660</td>\n",
       "      <td>12055</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do You Believe? (2015)</td>\n",
       "      <td>18</td>\n",
       "      <td>84</td>\n",
       "      <td>22</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>4.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>31</td>\n",
       "      <td>3136</td>\n",
       "      <td>1793</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hot Tub Time Machine 2 (2015)</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.4</td>\n",
       "      <td>...</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>88</td>\n",
       "      <td>19560</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FILM  RottenTomatoes  RottenTomatoes_User  \\\n",
       "0  Avengers: Age of Ultron (2015)              74                   86   \n",
       "1               Cinderella (2015)              85                   80   \n",
       "2                  Ant-Man (2015)              80                   90   \n",
       "3          Do You Believe? (2015)              18                   84   \n",
       "4   Hot Tub Time Machine 2 (2015)              14                   28   \n",
       "\n",
       "   Metacritic  Metacritic_User  IMDB  Fandango_Stars  Fandango_Ratingvalue  \\\n",
       "0          66              7.1   7.8             5.0                   4.5   \n",
       "1          67              7.5   7.1             5.0                   4.5   \n",
       "2          64              8.1   7.8             5.0                   4.5   \n",
       "3          22              4.7   5.4             5.0                   4.5   \n",
       "4          29              3.4   5.1             3.5                   3.0   \n",
       "\n",
       "   RT_norm  RT_user_norm  ...  IMDB_norm  RT_norm_round  RT_user_norm_round  \\\n",
       "0     3.70           4.3  ...       3.90            3.5                 4.5   \n",
       "1     4.25           4.0  ...       3.55            4.5                 4.0   \n",
       "2     4.00           4.5  ...       3.90            4.0                 4.5   \n",
       "3     0.90           4.2  ...       2.70            1.0                 4.0   \n",
       "4     0.70           1.4  ...       2.55            0.5                 1.5   \n",
       "\n",
       "   Metacritic_norm_round  Metacritic_user_norm_round  IMDB_norm_round  \\\n",
       "0                    3.5                         3.5              4.0   \n",
       "1                    3.5                         4.0              3.5   \n",
       "2                    3.0                         4.0              4.0   \n",
       "3                    1.0                         2.5              2.5   \n",
       "4                    1.5                         1.5              2.5   \n",
       "\n",
       "   Metacritic_user_vote_count  IMDB_user_vote_count  Fandango_votes  \\\n",
       "0                        1330                271107           14846   \n",
       "1                         249                 65709           12640   \n",
       "2                         627                103660           12055   \n",
       "3                          31                  3136            1793   \n",
       "4                          88                 19560            1021   \n",
       "\n",
       "   Fandango_Difference  \n",
       "0                  0.5  \n",
       "1                  0.5  \n",
       "2                  0.5  \n",
       "3                  0.5  \n",
       "4                  0.5  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read and display the CSV with Pandas\n",
    "movie_file_df = pd.read_csv(movie_file)\n",
    "movie_file_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FILM', 'RottenTomatoes', 'RottenTomatoes_User', 'Metacritic',\n",
       "       'Metacritic_User', 'IMDB', 'Fandango_Stars', 'Fandango_Ratingvalue',\n",
       "       'RT_norm', 'RT_user_norm', 'Metacritic_norm', 'Metacritic_user_nom',\n",
       "       'IMDB_norm', 'RT_norm_round', 'RT_user_norm_round',\n",
       "       'Metacritic_norm_round', 'Metacritic_user_norm_round',\n",
       "       'IMDB_norm_round', 'Metacritic_user_vote_count', 'IMDB_user_vote_count',\n",
       "       'Fandango_votes', 'Fandango_Difference'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all the columns in the table\n",
    "movie_file_df.columns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILM</th>\n",
       "      <th>IMDB</th>\n",
       "      <th>IMDB_norm</th>\n",
       "      <th>IMDB_norm_round</th>\n",
       "      <th>IMDB_user_vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avengers: Age of Ultron (2015)</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3.90</td>\n",
       "      <td>4.0</td>\n",
       "      <td>271107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cinderella (2015)</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.5</td>\n",
       "      <td>65709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ant-Man (2015)</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3.90</td>\n",
       "      <td>4.0</td>\n",
       "      <td>103660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do You Believe? (2015)</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hot Tub Time Machine 2 (2015)</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.5</td>\n",
       "      <td>19560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FILM  IMDB  IMDB_norm  IMDB_norm_round  \\\n",
       "0  Avengers: Age of Ultron (2015)   7.8       3.90              4.0   \n",
       "1               Cinderella (2015)   7.1       3.55              3.5   \n",
       "2                  Ant-Man (2015)   7.8       3.90              4.0   \n",
       "3          Do You Believe? (2015)   5.4       2.70              2.5   \n",
       "4   Hot Tub Time Machine 2 (2015)   5.1       2.55              2.5   \n",
       "\n",
       "   IMDB_user_vote_count  \n",
       "0                271107  \n",
       "1                 65709  \n",
       "2                103660  \n",
       "3                  3136  \n",
       "4                 19560  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only want IMDb data, so create a new table that takes the Film and all the columns relating to IMDB\n",
    "imdb_df = movie_file_df[[\"FILM\", \"IMDB\", \"IMDB_norm\",\n",
    "                            \"IMDB_norm_round\", \"IMDB_user_vote_count\"]]\n",
    "imdb_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILM</th>\n",
       "      <th>IMDB</th>\n",
       "      <th>IMDB_user_vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avengers: Age of Ultron (2015)</td>\n",
       "      <td>7.8</td>\n",
       "      <td>271107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cinderella (2015)</td>\n",
       "      <td>7.1</td>\n",
       "      <td>65709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ant-Man (2015)</td>\n",
       "      <td>7.8</td>\n",
       "      <td>103660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Water Diviner (2015)</td>\n",
       "      <td>7.2</td>\n",
       "      <td>39373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shaun the Sheep Movie (2015)</td>\n",
       "      <td>7.4</td>\n",
       "      <td>12227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FILM  IMDB  IMDB_user_vote_count\n",
       "0  Avengers: Age of Ultron (2015)   7.8                271107\n",
       "1               Cinderella (2015)   7.1                 65709\n",
       "2                  Ant-Man (2015)   7.8                103660\n",
       "5        The Water Diviner (2015)   7.2                 39373\n",
       "8    Shaun the Sheep Movie (2015)   7.4                 12227"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only like good movies, so find those that scored over 7, and ignore the norm rating\n",
    "good_movies_df = movie_file_df.loc[movie_file_df[\"IMDB\"] > 7, [\"FILM\", \"IMDB\", \"IMDB_user_vote_count\"]]\n",
    "\n",
    "good_movies_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILM</th>\n",
       "      <th>IMDB</th>\n",
       "      <th>IMDB_user_vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shaun the Sheep Movie (2015)</td>\n",
       "      <td>7.4</td>\n",
       "      <td>12227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Love &amp; Mercy (2015)</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Far From The Madding Crowd (2015)</td>\n",
       "      <td>7.2</td>\n",
       "      <td>12129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>McFarland, USA (2015)</td>\n",
       "      <td>7.5</td>\n",
       "      <td>13769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The End of the Tour (2015)</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 FILM  IMDB  IMDB_user_vote_count\n",
       "8        Shaun the Sheep Movie (2015)   7.4                 12227\n",
       "9                 Love & Mercy (2015)   7.8                  5367\n",
       "10  Far From The Madding Crowd (2015)   7.2                 12129\n",
       "20              McFarland, USA (2015)   7.5                 13769\n",
       "29         The End of the Tour (2015)   7.9                  1320"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find less popular movies--i.e., those with fewer than 20K votes\n",
    "unknown_movies_df = good_movies_df.loc[good_movies_df[\"IMDB_user_vote_count\"] < 20000, [\n",
    "    \"FILM\", \"IMDB\", \"IMDB_user_vote_count\"]]\n",
    "unknown_movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Activity 04 Solution ‚úÖ</strong></summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "# Dependencie\n",
    "import pandas as pd\n",
    "\n",
    "# Load in file\n",
    "movie_file = \"Resources/movie_scores.csv\"\n",
    "\n",
    "# Read and display the CSV with Pandas\n",
    "movie_file_df = pd.read_csv(movie_file)\n",
    "movie_file_df.head()\n",
    "\n",
    "# List all the columns in the table\n",
    "movie_file_df.columns\n",
    "\n",
    "\n",
    "# We only want IMDb data, so create a new table that takes the Film and all the columns relating to IMDB\n",
    "imdb_df = movie_file_df[[\"FILM\", \"IMDB\", \"IMDB_norm\",\n",
    "                            \"IMDB_norm_round\", \"IMDB_user_vote_count\"]]\n",
    "imdb_df.head()\n",
    "\n",
    "# We only like good movies, so find those that scored over 7, and ignore the norm rating\n",
    "good_movies_df = movie_file_df.loc[movie_file_df[\"IMDB\"] > 7, [\n",
    "    \"FILM\", \"IMDB\", \"IMDB_user_vote_count\"]]\n",
    "good_movies_df.head()\n",
    "\n",
    "# Find less popular movies--i.e., those with fewer than 20K votes\n",
    "unknown_movies_df = good_movies_df.loc[good_movies_df[\"IMDB_user_vote_count\"] < 20000, [\n",
    "    \"FILM\", \"IMDB\", \"IMDB_user_vote_count\"]]\n",
    "unknown_movies_df.head()\n",
    "\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn - 05 - GroupBy - üë©‚Äçüè´üßë‚Äçüè´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/1949 21:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>7200</td>\n",
       "      <td>1-2 hrs</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.38421</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/10/1955 17:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>20 seconds</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.2</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime                  city state country     shape  \\\n",
       "0  10/10/1949 20:30            san marcos    tx      us  cylinder   \n",
       "1  10/10/1949 21:00          lackland afb    tx     NaN     light   \n",
       "2  10/10/1955 17:00  chester (uk/england)   NaN      gb    circle   \n",
       "3  10/10/1956 21:00                  edna    tx      us    circle   \n",
       "4  10/10/1960 20:00               kaneohe    hi      us     light   \n",
       "\n",
       "  duration (seconds) duration (hours/min)  \\\n",
       "0               2700           45 minutes   \n",
       "1               7200              1-2 hrs   \n",
       "2                 20           20 seconds   \n",
       "3                 20             1/2 hour   \n",
       "4                900           15 minutes   \n",
       "\n",
       "                                            comments date posted    latitude  \\\n",
       "0  This event took place in early fall around 194...   4/27/2004  29.8830556   \n",
       "1  1949 Lackland AFB&#44 TX.  Lights racing acros...  12/16/2005    29.38421   \n",
       "2  Green/Orange circular disc over Chester&#44 En...   1/21/2008        53.2   \n",
       "3  My older brother and twin sister were leaving ...   1/17/2004  28.9783333   \n",
       "4  AS a Marine 1st Lt. flying an FJ4B fighter/att...   1/22/2004  21.4180556   \n",
       "\n",
       "   longitude   \n",
       "0  -97.941111  \n",
       "1  -98.581082  \n",
       "2   -2.916667  \n",
       "3  -96.645833  \n",
       "4 -157.803611  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Create a reference the CSV file desired\n",
    "csv_path = \"Resources/ufoSightings.csv\"\n",
    "\n",
    "# Read the CSV into a Pandas DataFrame\n",
    "ufo_df = pd.read_csv(csv_path, low_memory = False)\n",
    "\n",
    "# Print the first five rows of data to the screen\n",
    "ufo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                66516\n",
       "city                    66516\n",
       "state                   66516\n",
       "country                 66516\n",
       "shape                   66516\n",
       "duration (seconds)      66516\n",
       "duration (hours/min)    66516\n",
       "comments                66516\n",
       "date posted             66516\n",
       "latitude                66516\n",
       "longitude               66516\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the rows with missing data\n",
    "clean_ufo_df = ufo_df.dropna(how=\"any\")\n",
    "clean_ufo_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                 object\n",
       "city                     object\n",
       "state                    object\n",
       "country                  object\n",
       "shape                    object\n",
       "duration (seconds)       object\n",
       "duration (hours/min)     object\n",
       "comments                 object\n",
       "date posted              object\n",
       "latitude                 object\n",
       "longitude               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ufo_df.head()\n",
    "clean_ufo_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900.0</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10/10/1961 19:00</td>\n",
       "      <td>bristol</td>\n",
       "      <td>tn</td>\n",
       "      <td>us</td>\n",
       "      <td>sphere</td>\n",
       "      <td>300.0</td>\n",
       "      <td>5 minutes</td>\n",
       "      <td>My father is now 89 my brother 52 the girl wit...</td>\n",
       "      <td>4/27/2007</td>\n",
       "      <td>36.595</td>\n",
       "      <td>-82.188889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10/10/1965 23:45</td>\n",
       "      <td>norwalk</td>\n",
       "      <td>ct</td>\n",
       "      <td>us</td>\n",
       "      <td>disk</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>20 minutes</td>\n",
       "      <td>A bright orange color changing to reddish colo...</td>\n",
       "      <td>10/2/1999</td>\n",
       "      <td>41.1175</td>\n",
       "      <td>-73.408333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime        city state country     shape  duration (seconds)  \\\n",
       "0  10/10/1949 20:30  san marcos    tx      us  cylinder              2700.0   \n",
       "3  10/10/1956 21:00        edna    tx      us    circle                20.0   \n",
       "4  10/10/1960 20:00     kaneohe    hi      us     light               900.0   \n",
       "5  10/10/1961 19:00     bristol    tn      us    sphere               300.0   \n",
       "7  10/10/1965 23:45     norwalk    ct      us      disk              1200.0   \n",
       "\n",
       "  duration (hours/min)                                           comments  \\\n",
       "0           45 minutes  This event took place in early fall around 194...   \n",
       "3             1/2 hour  My older brother and twin sister were leaving ...   \n",
       "4           15 minutes  AS a Marine 1st Lt. flying an FJ4B fighter/att...   \n",
       "5            5 minutes  My father is now 89 my brother 52 the girl wit...   \n",
       "7           20 minutes  A bright orange color changing to reddish colo...   \n",
       "\n",
       "  date posted    latitude  longitude   \n",
       "0   4/27/2004  29.8830556  -97.941111  \n",
       "3   1/17/2004  28.9783333  -96.645833  \n",
       "4   1/22/2004  21.4180556 -157.803611  \n",
       "5   4/27/2007      36.595  -82.188889  \n",
       "7   10/2/1999     41.1175  -73.408333  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the \"duration (seconds)\" column's values to numeric\n",
    "converted_ufo = clean_ufo_df.copy()\n",
    "converted_ufo[\"duration (seconds)\"] = converted_ufo.loc[:, \"duration (seconds)\"].astype(float)\n",
    "\n",
    "converted_ufo.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900.0</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10/10/1961 19:00</td>\n",
       "      <td>bristol</td>\n",
       "      <td>tn</td>\n",
       "      <td>us</td>\n",
       "      <td>sphere</td>\n",
       "      <td>300.0</td>\n",
       "      <td>5 minutes</td>\n",
       "      <td>My father is now 89 my brother 52 the girl wit...</td>\n",
       "      <td>4/27/2007</td>\n",
       "      <td>36.595</td>\n",
       "      <td>-82.188889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10/10/1965 23:45</td>\n",
       "      <td>norwalk</td>\n",
       "      <td>ct</td>\n",
       "      <td>us</td>\n",
       "      <td>disk</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>20 minutes</td>\n",
       "      <td>A bright orange color changing to reddish colo...</td>\n",
       "      <td>10/2/1999</td>\n",
       "      <td>41.1175</td>\n",
       "      <td>-73.408333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime        city state country     shape  duration (seconds)  \\\n",
       "0  10/10/1949 20:30  san marcos    tx      us  cylinder              2700.0   \n",
       "3  10/10/1956 21:00        edna    tx      us    circle                20.0   \n",
       "4  10/10/1960 20:00     kaneohe    hi      us     light               900.0   \n",
       "5  10/10/1961 19:00     bristol    tn      us    sphere               300.0   \n",
       "7  10/10/1965 23:45     norwalk    ct      us      disk              1200.0   \n",
       "\n",
       "  duration (hours/min)                                           comments  \\\n",
       "0           45 minutes  This event took place in early fall around 194...   \n",
       "3             1/2 hour  My older brother and twin sister were leaving ...   \n",
       "4           15 minutes  AS a Marine 1st Lt. flying an FJ4B fighter/att...   \n",
       "5            5 minutes  My father is now 89 my brother 52 the girl wit...   \n",
       "7           20 minutes  A bright orange color changing to reddish colo...   \n",
       "\n",
       "  date posted    latitude  longitude   \n",
       "0   4/27/2004  29.8830556  -97.941111  \n",
       "3   1/17/2004  28.9783333  -96.645833  \n",
       "4   1/22/2004  21.4180556 -157.803611  \n",
       "5   4/27/2007      36.595  -82.188889  \n",
       "7   10/2/1999     41.1175  -73.408333  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the data so that only those sightings in the US are in a DataFrame\n",
    "usa_ufo_df = converted_ufo.loc[converted_ufo[\"country\"] == \"us\", :]\n",
    "usa_ufo_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ca    8683\n",
       "fl    3754\n",
       "wa    3707\n",
       "tx    3398\n",
       "ny    2915\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count how many sightings have occured within each state\n",
    "state_counts = usa_ufo_df[\"state\"].value_counts()\n",
    "state_counts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GroupBy in order to separate the data into fields according to \"state\" values\n",
    "grouped_usa_df = usa_ufo_df.groupby(['state'])\n",
    "\n",
    "# The object returned is a \"GroupBy\" object and cannot be viewed normally...\n",
    "print(grouped_usa_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to be visualized, a data function must be used...\n",
    "grouped_usa_df.count().head(10)\n",
    "\n",
    "grouped_usa_df[\"duration (seconds)\"].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since \"duration (seconds)\" was converted to a numeric time, it can now be summed up per state\n",
    "state_duration = grouped_usa_df[\"duration (seconds)\"].sum()\n",
    "state_duration.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame using both duration and count\n",
    "state_summary_table = pd.DataFrame({\"Number of Sightings\": state_counts,\n",
    "                                    \"Total Visit Time\": state_duration})\n",
    "state_summary_table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is also possible to group a DataFrame by multiple columns\n",
    "# This returns an object with multiple indexes, however, which can be harder to deal with\n",
    "grouped_international_data = converted_ufo.groupby(['country', 'state'])\n",
    "\n",
    "grouped_international_data.count().head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converting a GroupBy object into a DataFrame\n",
    "international_duration = pd.DataFrame(\n",
    "    grouped_international_data[\"duration (seconds)\"].sum())\n",
    "international_duration.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Turn - 06 - Training Grounds - üë©‚Äçüéìüë®‚Äçüéì\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Using the DataFrame provided, do the following:\n",
    "\n",
    "    * Convert the \"Membership (Days)\" column into weeks and then add this new series into the DataFrame\n",
    "\n",
    "    * Create a Dataframe that has only the \"Trainer\", \"Weight\", and membership in days and weeks.\n",
    "\n",
    "    * Using groupby get the average weight and length membership of the gym members for each trainer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A seriously gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members\n",
    "training_data = pd.DataFrame({\n",
    "    \"Name\":[\"Gino Walker\",\"Hiedi Wasser\",\"Kerrie Wetzel\",\"Elizabeth Sackett\",\"Jack Mitten\",\"Madalene Wayman\",\"Jamee Horvath\",\"Arlena Reddin\",\"Tula Levan\",\"Teisha Dreier\",\"Leslie Carrier\",\"Arlette Hartson\",\"Romana Merkle\",\"Heath Viviani\",\"Andres Zimmer\",\"Allyson Osman\",\"Yadira Caggiano\",\"Jeanmarie Friedrichs\",\"Leann Ussery\",\"Bee Mom\",\"Pandora Charland\",\"Karena Wooten\",\"Elizabet Albanese\",\"Augusta Borjas\",\"Erma Yadon\",\"Belia Lenser\",\"Karmen Sancho\",\"Edison Mannion\",\"Sonja Hornsby\",\"Morgan Frei\",\"Florencio Murphy\",\"Christoper Hertel\",\"Thalia Stepney\",\"Tarah Argento\",\"Nicol Canfield\",\"Pok Moretti\",\"Barbera Stallings\",\"Muoi Kelso\",\"Cicely Ritz\",\"Sid Demelo\",\"Eura Langan\",\"Vanita An\",\"Frieda Fuhr\",\"Ernest Fitzhenry\",\"Ashlyn Tash\",\"Melodi Mclendon\",\"Rochell Leblanc\",\"Jacqui Reasons\",\"Freeda Mccroy\",\"Vanna Runk\",\"Florinda Milot\",\"Cierra Lecompte\",\"Nancey Kysar\",\"Latasha Dalton\",\"Charlyn Rinaldi\",\"Erline Averett\",\"Mariko Hillary\",\"Rosalyn Trigg\",\"Sherwood Brauer\",\"Hortencia Olesen\",\"Delana Kohut\",\"Geoffrey Mcdade\",\"Iona Delancey\",\"Donnie Read\",\"Cesar Bhatia\",\"Evia Slate\",\"Kaye Hugo\",\"Denise Vento\",\"Lang Kittle\",\"Sherry Whittenberg\",\"Jodi Bracero\",\"Tamera Linneman\",\"Katheryn Koelling\",\"Tonia Shorty\",\"Misha Baxley\",\"Lisbeth Goering\",\"Merle Ladwig\",\"Tammie Omar\",\"Jesusa Avilla\",\"Alda Zabala\",\"Junita Dogan\",\"Jessia Anglin\",\"Peggie Scranton\",\"Dania Clodfelter\",\"Janis Mccarthy\",\"Edmund Galusha\",\"Tonisha Posey\",\"Arvilla Medley\",\"Briana Barbour\",\"Delfina Kiger\",\"Nia Lenig\",\"Ricarda Bulow\",\"Odell Carson\",\"Nydia Clonts\",\"Andree Resendez\",\"Daniela Puma\",\"Sherill Paavola\",\"Gilbert Bloomquist\",\"Shanon Mach\",\"Justin Bangert\",\"Arden Hokanson\",\"Evelyne Bridge\",\"Hee Simek\",\"Ward Deangelis\",\"Jodie Childs\",\"Janis Boehme\",\"Beaulah Glowacki\",\"Denver Stoneham\",\"Tarra Vinton\",\"Deborah Hummell\",\"Ulysses Neil\",\"Kathryn Marques\",\"Rosanna Dake\",\"Gavin Wheat\",\"Tameka Stoke\",\"Janella Clear\",\"Kaye Ciriaco\",\"Suk Bloxham\",\"Gracia Whaley\",\"Philomena Hemingway\",\"Claudette Vaillancourt\",\"Olevia Piche\",\"Trey Chiles\",\"Idalia Scardina\",\"Jenine Tremble\",\"Herbert Krider\",\"Alycia Schrock\",\"Miss Weibel\",\"Pearlene Neidert\",\"Kina Callender\",\"Charlotte Skelley\",\"Theodora Harrigan\",\"Sydney Shreffler\",\"Annamae Trinidad\",\"Tobi Mumme\",\"Rosia Elliot\",\"Debbra Putt\",\"Rena Delosantos\",\"Genna Grennan\",\"Nieves Huf\",\"Berry Lugo\",\"Ayana Verdugo\",\"Joaquin Mazzei\",\"Doris Harmon\",\"Patience Poss\",\"Magaret Zabel\",\"Marylynn Hinojos\",\"Earlene Marcantel\",\"Yuki Evensen\",\"Rema Gay\",\"Delana Haak\",\"Patricia Fetters\",\"Vinnie Elrod\",\"Octavia Bellew\",\"Burma Revard\",\"Lakenya Kato\",\"Vinita Buchner\",\"Sierra Margulies\",\"Shae Funderburg\",\"Jenae Groleau\",\"Louetta Howie\",\"Astrid Duffer\",\"Caron Altizer\",\"Kymberly Amavisca\",\"Mohammad Diedrich\",\"Thora Wrinkle\",\"Bethel Wiemann\",\"Patria Millet\",\"Eldridge Burbach\",\"Alyson Eddie\",\"Zula Hanna\",\"Devin Goodwin\",\"Felipa Kirkwood\",\"Kurtis Kempf\",\"Kasey Lenart\",\"Deena Blankenship\",\"Kandra Wargo\",\"Sherrie Cieslak\",\"Ron Atha\",\"Reggie Barreiro\",\"Daria Saulter\",\"Tandra Eastman\",\"Donnell Lucious\",\"Talisha Rosner\",\"Emiko Bergh\",\"Terresa Launius\",\"Margy Hoobler\",\"Marylou Stelling\",\"Lavonne Justice\",\"Kala Langstaff\",\"China Truett\",\"Louanne Dussault\",\"Thomasena Samaniego\",\"Charlesetta Tarbell\",\"Fatimah Lade\",\"Malisa Cantero\",\"Florencia Litten\",\"Francina Fraise\",\"Patsy London\",\"Deloris Mclaughlin\"],\n",
    "    \"Trainer\":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],\n",
    "    \"Weight\":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],\n",
    "    \"Membership (Days)\":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]\n",
    "})\n",
    "training_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the membership days into weeks and then add a this data in a new column to the DataFrame.\n",
    "\n",
    "# Create a Dataframe that has the Trainer, Weight, and Membership.\n",
    "\n",
    "# Using groupby get the average weight and length membership for each trainer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Activity 06 Solution ‚úÖ</strong></summary>\n",
    "    \n",
    "```python\n",
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# A seriously gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members\n",
    "training_data = pd.DataFrame({\n",
    "    \"Name\":[\"Gino Walker\",\"Hiedi Wasser\",\"Kerrie Wetzel\",\"Elizabeth Sackett\",\"Jack Mitten\",\"Madalene Wayman\",\"Jamee Horvath\",\"Arlena Reddin\",\"Tula Levan\",\"Teisha Dreier\",\"Leslie Carrier\",\"Arlette Hartson\",\"Romana Merkle\",\"Heath Viviani\",\"Andres Zimmer\",\"Allyson Osman\",\"Yadira Caggiano\",\"Jeanmarie Friedrichs\",\"Leann Ussery\",\"Bee Mom\",\"Pandora Charland\",\"Karena Wooten\",\"Elizabet Albanese\",\"Augusta Borjas\",\"Erma Yadon\",\"Belia Lenser\",\"Karmen Sancho\",\"Edison Mannion\",\"Sonja Hornsby\",\"Morgan Frei\",\"Florencio Murphy\",\"Christoper Hertel\",\"Thalia Stepney\",\"Tarah Argento\",\"Nicol Canfield\",\"Pok Moretti\",\"Barbera Stallings\",\"Muoi Kelso\",\"Cicely Ritz\",\"Sid Demelo\",\"Eura Langan\",\"Vanita An\",\"Frieda Fuhr\",\"Ernest Fitzhenry\",\"Ashlyn Tash\",\"Melodi Mclendon\",\"Rochell Leblanc\",\"Jacqui Reasons\",\"Freeda Mccroy\",\"Vanna Runk\",\"Florinda Milot\",\"Cierra Lecompte\",\"Nancey Kysar\",\"Latasha Dalton\",\"Charlyn Rinaldi\",\"Erline Averett\",\"Mariko Hillary\",\"Rosalyn Trigg\",\"Sherwood Brauer\",\"Hortencia Olesen\",\"Delana Kohut\",\"Geoffrey Mcdade\",\"Iona Delancey\",\"Donnie Read\",\"Cesar Bhatia\",\"Evia Slate\",\"Kaye Hugo\",\"Denise Vento\",\"Lang Kittle\",\"Sherry Whittenberg\",\"Jodi Bracero\",\"Tamera Linneman\",\"Katheryn Koelling\",\"Tonia Shorty\",\"Misha Baxley\",\"Lisbeth Goering\",\"Merle Ladwig\",\"Tammie Omar\",\"Jesusa Avilla\",\"Alda Zabala\",\"Junita Dogan\",\"Jessia Anglin\",\"Peggie Scranton\",\"Dania Clodfelter\",\"Janis Mccarthy\",\"Edmund Galusha\",\"Tonisha Posey\",\"Arvilla Medley\",\"Briana Barbour\",\"Delfina Kiger\",\"Nia Lenig\",\"Ricarda Bulow\",\"Odell Carson\",\"Nydia Clonts\",\"Andree Resendez\",\"Daniela Puma\",\"Sherill Paavola\",\"Gilbert Bloomquist\",\"Shanon Mach\",\"Justin Bangert\",\"Arden Hokanson\",\"Evelyne Bridge\",\"Hee Simek\",\"Ward Deangelis\",\"Jodie Childs\",\"Janis Boehme\",\"Beaulah Glowacki\",\"Denver Stoneham\",\"Tarra Vinton\",\"Deborah Hummell\",\"Ulysses Neil\",\"Kathryn Marques\",\"Rosanna Dake\",\"Gavin Wheat\",\"Tameka Stoke\",\"Janella Clear\",\"Kaye Ciriaco\",\"Suk Bloxham\",\"Gracia Whaley\",\"Philomena Hemingway\",\"Claudette Vaillancourt\",\"Olevia Piche\",\"Trey Chiles\",\"Idalia Scardina\",\"Jenine Tremble\",\"Herbert Krider\",\"Alycia Schrock\",\"Miss Weibel\",\"Pearlene Neidert\",\"Kina Callender\",\"Charlotte Skelley\",\"Theodora Harrigan\",\"Sydney Shreffler\",\"Annamae Trinidad\",\"Tobi Mumme\",\"Rosia Elliot\",\"Debbra Putt\",\"Rena Delosantos\",\"Genna Grennan\",\"Nieves Huf\",\"Berry Lugo\",\"Ayana Verdugo\",\"Joaquin Mazzei\",\"Doris Harmon\",\"Patience Poss\",\"Magaret Zabel\",\"Marylynn Hinojos\",\"Earlene Marcantel\",\"Yuki Evensen\",\"Rema Gay\",\"Delana Haak\",\"Patricia Fetters\",\"Vinnie Elrod\",\"Octavia Bellew\",\"Burma Revard\",\"Lakenya Kato\",\"Vinita Buchner\",\"Sierra Margulies\",\"Shae Funderburg\",\"Jenae Groleau\",\"Louetta Howie\",\"Astrid Duffer\",\"Caron Altizer\",\"Kymberly Amavisca\",\"Mohammad Diedrich\",\"Thora Wrinkle\",\"Bethel Wiemann\",\"Patria Millet\",\"Eldridge Burbach\",\"Alyson Eddie\",\"Zula Hanna\",\"Devin Goodwin\",\"Felipa Kirkwood\",\"Kurtis Kempf\",\"Kasey Lenart\",\"Deena Blankenship\",\"Kandra Wargo\",\"Sherrie Cieslak\",\"Ron Atha\",\"Reggie Barreiro\",\"Daria Saulter\",\"Tandra Eastman\",\"Donnell Lucious\",\"Talisha Rosner\",\"Emiko Bergh\",\"Terresa Launius\",\"Margy Hoobler\",\"Marylou Stelling\",\"Lavonne Justice\",\"Kala Langstaff\",\"China Truett\",\"Louanne Dussault\",\"Thomasena Samaniego\",\"Charlesetta Tarbell\",\"Fatimah Lade\",\"Malisa Cantero\",\"Florencia Litten\",\"Francina Fraise\",\"Patsy London\",\"Deloris Mclaughlin\"],\n",
    "    \"Trainer\":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],\n",
    "    \"Weight\":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],\n",
    "    \"Membership (Days)\":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]\n",
    "})\n",
    "training_data.head()\n",
    "\n",
    "# Convert the membership days into weeks and then add a this data in a new column to the DataFrame.\n",
    "weeks = training_data[\"Membership (Days)\"]/7\n",
    "training_data[\"Membership (Weeks)\"] = weeks\n",
    "\n",
    "training_data.head()\n",
    "\n",
    "# Create a Dataframe that has the Trainer, Weight, and Membership.\n",
    "trainers_data =  training_data[[\"Trainer\", \"Weight\", \"Membership (Days)\", \"Membership (Weeks)\"]]\n",
    "trainers_data\n",
    "\n",
    "# Using groupby get the average weight and length membership for each trainer.\n",
    "\n",
    "trainers_means = trainers_data.groupby([\"Trainer\"]).mean()\n",
    "trainers_means\n",
    "\n",
    "trainers_means.sort_values(by='Membership (Days)', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn - 07 - Binning - üë©‚Äçüè´üßë‚Äçüè´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Load in file\n",
    "final_exam_scores = \"resources/final_exam_scores.csv\"\n",
    "\n",
    "df = pd.read_csv(final_exam_scores)\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the bins in which Data will be held\n",
    "# Bins are 0, 59, 69, 79, 89, 100.   \n",
    "bins = [0, 59, 69, 79, 89, 100]\n",
    "\n",
    "# Create the names for the four bins\n",
    "group_names = [\"F\", \"D\", \"C\", \"B\", \"A\"]\n",
    "\n",
    "df[\"Grade\"] = pd.cut(df[\"Final Exam\"], bins, labels=group_names)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the summary statitics of each letter grade. \n",
    "df = df.groupby(\"Grade\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Turn - 08 - Binning Ted - üë©‚Äçüéìüë®‚Äçüéì\n",
    "## Binning TED\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Read in the CSV file provided and print it to the screen.\n",
    "\n",
    "* Find the minimum \"views\" and maximum \"views\".\n",
    "\n",
    "* Using the minimum and maximum \"views\" as a reference, create 10 bins in which to slice the data.\n",
    "\n",
    "* Create a new column called \"View Group\" and fill it with the values collected through your slicing.\n",
    "\n",
    "* Group the DataFrame based upon the values within \"View Group\".\n",
    "\n",
    "* Find out how many rows fall into each group before finding the averages for \"comments\", \"duration\", and \"languages\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4553</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>1164</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>60</td>\n",
       "      <td>Ken Robinson</td>\n",
       "      <td>Ken Robinson: Do schools kill creativity?</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>47227110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>265</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>977</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>43</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>3200520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>1286</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>26</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>David Pogue: Simplicity sells</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>1636292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>1116</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>35</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>Majora Carter: Greening the ghetto</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>1697550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>593</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>1190</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>48</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>Hans Rosling: The best stats you've ever seen</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>12005869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments                                        description  duration  \\\n",
       "0      4553  Sir Ken Robinson makes an entertaining and pro...      1164   \n",
       "1       265  With the same humor and humanity he exuded in ...       977   \n",
       "2       124  New York Times columnist David Pogue takes aim...      1286   \n",
       "3       200  In an emotionally charged talk, MacArthur-winn...      1116   \n",
       "4       593  You've never seen data presented like this. Wi...      1190   \n",
       "\n",
       "     event  languages   main_speaker  \\\n",
       "0  TED2006         60   Ken Robinson   \n",
       "1  TED2006         43        Al Gore   \n",
       "2  TED2006         26    David Pogue   \n",
       "3  TED2006         35  Majora Carter   \n",
       "4  TED2006         48   Hans Rosling   \n",
       "\n",
       "                                            name  \\\n",
       "0      Ken Robinson: Do schools kill creativity?   \n",
       "1           Al Gore: Averting the climate crisis   \n",
       "2                  David Pogue: Simplicity sells   \n",
       "3             Majora Carter: Greening the ghetto   \n",
       "4  Hans Rosling: The best stats you've ever seen   \n",
       "\n",
       "                             title     views  \n",
       "0      Do schools kill creativity?  47227110  \n",
       "1      Averting the climate crisis   3200520  \n",
       "2                 Simplicity sells   1636292  \n",
       "3              Greening the ghetto   1697550  \n",
       "4  The best stats you've ever seen  12005869  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Create a path to the csv and read it into a Pandas DataFrame\n",
    "csv_path = \"Resources/ted_talks.csv\"\n",
    "ted_df = pd.read_csv(csv_path)\n",
    "\n",
    "ted_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out the minimum and maximum views for a TED Talk\n",
    "\n",
    "# Create bins in which to place values based upon TED Talk views\n",
    "\n",
    "# Create labels for these bins\n",
    "\n",
    "# Slice the data and place it into bins\n",
    "\n",
    "# Place the data series into a new column inside of the DataFrame\n",
    "\n",
    "# Create a GroupBy object based upon \"View Group\"\n",
    "\n",
    "# Find how many rows fall into each bin\n",
    "\n",
    "# Get the average of each column within the GroupBy object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Activity 08 Solution ‚úÖ</strong></summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Create a path to the csv and read it into a Pandas DataFrame\n",
    "csv_path = \"Resources/ted_talks.csv\"\n",
    "ted_df = pd.read_csv(csv_path)\n",
    "\n",
    "ted_df.head()\n",
    "\n",
    "# Figure out the minimum and maximum views for a TED Talk\n",
    "print(ted_df[\"views\"].max())\n",
    "print(ted_df[\"views\"].min())\n",
    "\n",
    "# Create bins in which to place values based upon TED Talk views\n",
    "bins = [0, 199999, 399999, 599999, 799999, 999999,\n",
    "        1999999, 2999999, 3999999, 4999999, 50000000]\n",
    "\n",
    "# Create labels for these bins\n",
    "group_labels = [\"0 to 199k\", \"200k to 399k\", \"400k to 599k\", \"600k to 799k\", \"800k to 999k\", \"1mil to 2mil\",\n",
    "                \"2mil to 3mil\", \"3mil to 4mil\", \"4mil to 5mil\", \"5mil to 50mil\"]\n",
    "\n",
    "# Slice the data and place it into bins\n",
    "pd.cut(ted_df[\"views\"], bins, labels=group_labels).head()\n",
    "\n",
    "# Place the data series into a new column inside of the DataFrame\n",
    "ted_df[\"View Group\"] = pd.cut(ted_df[\"views\"], bins, labels=group_labels)\n",
    "ted_df.head()\n",
    "\n",
    "# Create a GroupBy object based upon \"View Group\"\n",
    "ted_group = ted_df.groupby(\"View Group\")\n",
    "\n",
    "# Find how many rows fall into each bin\n",
    "print(ted_group[\"comments\"].count())\n",
    "\n",
    "# Get the average of each column within the GroupBy object\n",
    "ted_group[[\"comments\", \"duration\", \"languages\"]].mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
